{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Necessary Libraries**"
      ],
      "metadata": {
        "id": "lC6TdIMo0_xN"
      },
      "id": "lC6TdIMo0_xN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e5757bd",
      "metadata": {
        "id": "5e5757bd"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import random as rn\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "\n",
        "np.random.seed(30)\n",
        "rn.seed(30)\n",
        "tf.random.set_seed(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìå What‚Äôs Happening:\n",
        "\n",
        "### 1. **Imports**:\n",
        "- `numpy`: For numerical computations.\n",
        "- `random`: Python‚Äôs built-in module for generating random values.\n",
        "- `tensorflow`: For building and training deep learning models.\n",
        "- `datetime`: (Not used in the snippet, but often used for timestamps or logging).\n",
        "\n",
        "---\n",
        "\n",
        "## üîê Why Set Seeds?\n",
        "\n",
        "Setting seeds ensures **reproducibility** in machine learning and deep learning workflows. When your code uses randomness‚Äîlike shuffling data, initializing weights, or applying dropout‚Äîsetting a seed guarantees the same output every time you run the script.\n",
        "\n",
        "This is especially useful for:\n",
        "- Debugging model behavior\n",
        "- Comparing model performance across runs\n",
        "- Collaborating with others or publishing reproducible research\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Breakdown of Seed Settings:\n",
        "\n",
        "| Line | Purpose |\n",
        "|------|---------|\n",
        "| `np.random.seed(30)` | Controls the randomness from NumPy (e.g., random numbers, shuffling) |\n",
        "| `rn.seed(30)` | Controls randomness from Python‚Äôs built-in `random` module |\n",
        "| `tf.random.set_seed(30)` | Controls randomness in TensorFlow (e.g., weight initialization, dropout) |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4CPHnSuB2jKF"
      },
      "id": "4CPHnSuB2jKF"
    },
    {
      "cell_type": "markdown",
      "id": "9aef6956",
      "metadata": {
        "id": "9aef6956"
      },
      "source": [
        "## 1. Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf59c327",
      "metadata": {
        "id": "cf59c327"
      },
      "outputs": [],
      "source": [
        "# Load Dataset and Set Paths\n",
        "\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GRU, Conv3D, MaxPooling3D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìå What‚Äôs Happening:\n",
        "\n",
        "### 1. **Library Imports Overview**:\n",
        "\n",
        "These imports prepare your deep learning script for **image processing**, **model building**, **optimization**, and **data handling**.\n",
        "\n",
        "---\n",
        "\n",
        "### üìÅ `import os`\n",
        "- Provides functions to **interact with the operating system**.\n",
        "- Commonly used to read file paths, list image folders, etc.\n",
        "- Example: `os.listdir('images/')` lists all files in the `images` folder.\n",
        "\n",
        "---\n",
        "\n",
        "### üñºÔ∏è `from PIL import Image`\n",
        "- PIL (Python Imaging Library) is used for **opening, manipulating, and saving images**.\n",
        "- You can resize, convert to grayscale, or apply filters.\n",
        "- Example: `Image.open('img.jpg').resize((128, 128))`\n",
        "\n",
        "---\n",
        "\n",
        "### ü§ñ `from tensorflow.keras.models import Sequential`\n",
        "- `Sequential` is a **linear stack of layers** ‚Äî perfect for most deep learning models where layers are added one after another.\n",
        "- You define your model layer-by-layer using this.\n",
        "\n",
        "---\n",
        "\n",
        "### üß± `from tensorflow.keras.layers import ...`\n",
        "Brings in key **building blocks for neural networks**:\n",
        "\n",
        "| Layer | Purpose |\n",
        "|-------|---------|\n",
        "| `Conv2D` | 2D Convolution layer (used for image data) |\n",
        "| `MaxPooling2D` | Downsamples image data after convolution |\n",
        "| `Flatten` | Converts 2D/3D outputs into a flat vector for Dense layers |\n",
        "| `Dense` | Fully connected layer, used for classification or regression |\n",
        "| `GRU` | Gated Recurrent Unit ‚Äì used for sequence data like time series or videos |\n",
        "| `Conv3D` | 3D Convolution (used for video or volumetric data) |\n",
        "| `MaxPooling3D` | 3D version of pooling for downsampling video/3D data |\n",
        "| `Dropout` | Prevents overfitting by randomly turning off neurons during training |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è `from tensorflow.keras.optimizers import Adam`\n",
        "- Adam is an **adaptive optimizer** used to train deep learning models.\n",
        "- It adjusts the learning rate during training.\n",
        "- Often works well out of the box.\n",
        "\n",
        "---\n",
        "\n",
        "### üî¢ `from tensorflow.keras.utils import to_categorical`\n",
        "- Converts integer labels (e.g., 0, 1, 2) into **one-hot encoded vectors**.\n",
        "- Useful for multi-class classification.\n",
        "- Example: `to_categorical([0, 1, 2]) ‚Üí [[1,0,0], [0,1,0], [0,0,1]]`\n",
        "\n",
        "---\n",
        "\n",
        "### üìä `import pandas as pd`\n",
        "- Imports **Pandas**, a powerful library for **data manipulation and analysis**.\n",
        "- Useful for reading CSV files, data preprocessing, and analysis.\n",
        "- Example: `pd.read_csv('labels.csv')`\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Final Takeaway:\n",
        "\n",
        "These imports collectively enable:\n",
        "- File and image handling (`os`, `PIL`)\n",
        "- Model architecture design (`Sequential`, `layers`)\n",
        "- Training and optimization (`Adam`, `Dropout`)\n",
        "- Label encoding (`to_categorical`)\n",
        "- Tabular data operations (`pandas`)\n",
        "\n",
        "You now have all the foundational tools to build and train an image/video classification deep learning model. üöÄ\n"
      ],
      "metadata": {
        "id": "lC5CNIeB3EAo"
      },
      "id": "lC5CNIeB3EAo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "981aa1f5",
      "metadata": {
        "id": "981aa1f5"
      },
      "outputs": [],
      "source": [
        "# Define the paths to gesture image datasets\n",
        "\n",
        "base_dir = '/home/datasets/Project_data'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "740c2c92",
      "metadata": {
        "id": "740c2c92",
        "outputId": "f3204d96-de23-4b2c-8916-e5cba611e7ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train CSV Sample:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>WIN_20180925_17_08_43_Pro_Left_Swipe_new;Left_Swipe_new;0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>WIN_20180925_17_18_28_Pro_Left_Swipe_new;Left_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>WIN_20180925_17_18_56_Pro_Left_Swipe_new;Left_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>WIN_20180925_17_19_51_Pro_Left_Swipe_new;Left_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>WIN_20180925_17_20_14_Pro_Left_Swipe_new;Left_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>WIN_20180925_17_21_28_Pro_Left_Swipe_new;Left_...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  WIN_20180925_17_08_43_Pro_Left_Swipe_new;Left_Swipe_new;0\n",
              "0  WIN_20180925_17_18_28_Pro_Left_Swipe_new;Left_...       \n",
              "1  WIN_20180925_17_18_56_Pro_Left_Swipe_new;Left_...       \n",
              "2  WIN_20180925_17_19_51_Pro_Left_Swipe_new;Left_...       \n",
              "3  WIN_20180925_17_20_14_Pro_Left_Swipe_new;Left_...       \n",
              "4  WIN_20180925_17_21_28_Pro_Left_Swipe_new;Left_...       "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation CSV Sample:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>WIN_20180925_17_17_04_Pro_Left_Swipe_new;Left_Swipe_new;0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>WIN_20180925_17_43_01_Pro_Left_Swipe_new;Left_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>WIN_20180925_18_01_40_Pro_Left_Swipe_new;Left_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>WIN_20180925_18_03_21_Pro_Left_Swipe_new;Left_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>WIN_20180926_16_46_22_Pro_Left_Swipe_new;Left_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>WIN_20180926_16_47_09_Pro_Left_Swipe_new;Left_...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  WIN_20180925_17_17_04_Pro_Left_Swipe_new;Left_Swipe_new;0\n",
              "0  WIN_20180925_17_43_01_Pro_Left_Swipe_new;Left_...       \n",
              "1  WIN_20180925_18_01_40_Pro_Left_Swipe_new;Left_...       \n",
              "2  WIN_20180925_18_03_21_Pro_Left_Swipe_new;Left_...       \n",
              "3  WIN_20180926_16_46_22_Pro_Left_Swipe_new;Left_...       \n",
              "4  WIN_20180926_16_47_09_Pro_Left_Swipe_new;Left_...       "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Read and load train and val csv\n",
        "train_csv = pd.read_csv(os.path.join(base_dir, 'train.csv'))\n",
        "val_csv = pd.read_csv(os.path.join(base_dir, 'val.csv'))\n",
        "\n",
        "print(\"Train CSV Sample:\")\n",
        "display(train_csv.head())\n",
        "print(\"Validation CSV Sample:\")\n",
        "display(val_csv.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6991516b",
      "metadata": {
        "id": "6991516b"
      },
      "outputs": [],
      "source": [
        "# Data Preparation and Batch Size Definition\n",
        "\n",
        "# Randomly shuffle training and validation data entries\n",
        "# This helps to mix the dataset for each epoch, improving training efficiency.\n",
        "\n",
        "train_doc = np.random.permutation(open('/home/datasets/Project_data/train.csv').readlines())\n",
        "val_doc = np.random.permutation(open('/home/datasets/Project_data/val.csv').readlines())\n",
        "\n",
        "# Set batch size for model training\n",
        "# Batch size controls the number of samples processed before updating the model weights.\n",
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "```markdown\n",
        "## üì¶ Data Preparation and Batch Size Definition\n",
        "\n",
        "### üîÄ Randomly Shuffle Training and Validation Data\n",
        "\n",
        "```python\n",
        "train_doc = np.random.permutation(open('/home/datasets/Project_data/train.csv').readlines())\n",
        "val_doc = np.random.permutation(open('/home/datasets/Project_data/val.csv').readlines())\n",
        "```\n",
        "\n",
        "#### ‚úÖ What this does:\n",
        "- Loads all lines (entries) from the `train.csv` and `val.csv` files.\n",
        "- Each line typically contains a file path and a label (e.g., image/video path + class).\n",
        "- `np.random.permutation()` shuffles the order of these entries randomly.\n",
        "\n",
        "#### üí° Why shuffle?\n",
        "- To **reduce model bias** caused by any ordering in the data.\n",
        "- To ensure each **epoch sees the data in a new order**, improving generalization.\n",
        "- Shuffling is a **standard practice** before feeding data into the model during training.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è Define Batch Size\n",
        "\n",
        "```python\n",
        "batch_size = 16\n",
        "```\n",
        "\n",
        "#### ‚úÖ What is batch size?\n",
        "- The number of **training samples** the model processes **before updating the weights**.\n",
        "- A batch size of `16` means:\n",
        "  - The model processes 16 samples at once.\n",
        "  - Gradients are computed and averaged over these 16 samples.\n",
        "  - One weight update is performed per batch.\n",
        "\n",
        "#### üìä Why 16?\n",
        "- It's a commonly used batch size that balances:\n",
        "  - Training speed\n",
        "  - Memory efficiency (especially on GPUs)\n",
        "  - Model convergence\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Summary\n",
        "\n",
        "| Component | Purpose |\n",
        "|----------|---------|\n",
        "| `np.random.permutation(...)` | Randomly shuffles the dataset lines for better training |\n",
        "| `batch_size = 16` | Controls how many samples are processed per training step |\n",
        "\n"
      ],
      "metadata": {
        "id": "5DsjVE0m4AlR"
      },
      "id": "5DsjVE0m4AlR"
    },
    {
      "cell_type": "markdown",
      "id": "986f67d1",
      "metadata": {
        "id": "986f67d1"
      },
      "source": [
        "## 2. Generator Function for Batch Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77c0e410",
      "metadata": {
        "id": "77c0e410"
      },
      "outputs": [],
      "source": [
        "# Generator Function for Batch Data Loading and Preprocessing\n",
        "\n",
        "# This function is a generator that loads image data in batches for model training.\n",
        "# It shuffles the data, resizes images, normalizes pixel values, and one-hot encodes labels.\n",
        "# The generator yields batches of data and labels, which is memory efficient for large datasets.\n",
        "\n",
        "# Infinite loop to continuously yield batches for training\n",
        "\n",
        "def generator(source_path, folder_list, batch_size, img_size=(100, 100), num_classes=5):\n",
        "    # Infinite loop to continuously yield batches for training\n",
        "    print('Source path =', source_path, '; batch size =', batch_size)\n",
        "\n",
        "    img_idx = list(range(30))\n",
        "    x = len(img_idx)\n",
        "    y, z = img_size\n",
        "\n",
        "    # Infinite loop to continuously yield batches for training\n",
        "    while True:\n",
        "        t = np.random.permutation(folder_list)\n",
        "        num_batches = len(t) // batch_size\n",
        "\n",
        "\n",
        "         # Loop through each batch\n",
        "        for batch in range(num_batches):\n",
        "            batch_data = np.zeros((batch_size, x, y, z, 3))\n",
        "            batch_labels = np.zeros((batch_size, num_classes))\n",
        "\n",
        "            # Load and process images for each sample in the batch\n",
        "            for folder in range(batch_size):\n",
        "                folder_name = t[folder + (batch * batch_size)].strip().split(';')[0]\n",
        "                folder_path = os.path.join(source_path, folder_name)\n",
        "                imgs = sorted(os.listdir(folder_path))\n",
        "\n",
        "                # Load each frame, resize, normalize, and add to batch\n",
        "                for idx, item in enumerate(img_idx):\n",
        "                    img_path = os.path.join(folder_path, imgs[item])\n",
        "                    image = Image.open(img_path).resize((y, z))\n",
        "                    image_array = np.array(image).astype(np.float32) / 255.0\n",
        "                    if image_array.shape[-1] == 3:\n",
        "                        batch_data[folder, idx, :, :, :] = image_array\n",
        "                    else:\n",
        "                        batch_data[folder, idx, :, :, :] = np.stack([image_array]*3, axis=-1)\n",
        "\n",
        "                # Parse label from folder list and one-hot encode it\n",
        "                label = int(t[folder + (batch * batch_size)].strip().split(';')[2])\n",
        "                batch_labels[folder] = to_categorical(label, num_classes=num_classes)\n",
        "\n",
        "             # Yield the current batch of data and labels\n",
        "            yield batch_data, batch_labels\n",
        "\n",
        "        # Check if the image has 3 color channels; if not, replicate grayscale channel\n",
        "        # Handle remaining samples if total data is not a multiple of the batch size\n",
        "        if len(t) % batch_size != 0:\n",
        "            remaining_size = len(t) % batch_size\n",
        "            batch_data = np.zeros((remaining_size, x, y, z, 3))\n",
        "            batch_labels = np.zeros((remaining_size, num_classes))\n",
        "\n",
        "            for folder in range(remaining_size):\n",
        "                folder_name = t[folder + (num_batches * batch_size)].strip().split(';')[0]\n",
        "                folder_path = os.path.join(source_path, folder_name)\n",
        "                imgs = sorted(os.listdir(folder_path))\n",
        "\n",
        "                for idx, item in enumerate(img_idx):\n",
        "                    img_path = os.path.join(folder_path, imgs[item])\n",
        "                    image = Image.open(img_path).resize((y, z))\n",
        "                    image_array = np.array(image).astype(np.float32) / 255.0\n",
        "                    if image_array.shape[-1] == 3:\n",
        "                        batch_data[folder, idx, :, :, :] = image_array\n",
        "                    else:\n",
        "                        batch_data[folder, idx, :, :, :] = np.stack([image_array]*3, axis=-1)\n",
        "\n",
        "                label = int(t[folder + (num_batches * batch_size)].strip().split(';')[2])\n",
        "                batch_labels[folder] = to_categorical(label, num_classes=num_classes)\n",
        "\n",
        "            yield batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```markdown\n",
        "# **üéØ Generator Function for Batch Data Loading and Preprocessing**\n",
        "\n",
        "This generator function is used to efficiently load and preprocess image sequence data (like frames from a video) during model training.\n",
        "\n",
        "It works in batches, performs real-time augmentation, normalization, and label encoding‚Äî**essential for large datasets where loading everything into memory isn't feasible**.\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Function Signature**\n",
        "```python\n",
        "def generator(source_path, folder_list, batch_size, img_size=(100, 100), num_classes=5):\n",
        "```\n",
        "- `source_path`: Directory containing all sample folders (each folder = 1 video sample).\n",
        "- `folder_list`: A list where each item has format like `\"folder_name;label\"`.\n",
        "- `batch_size`: Number of samples per training batch.\n",
        "- `img_size`: Tuple of (height, width) to resize images.\n",
        "- `num_classes`: Total number of categories for classification.\n",
        "\n",
        "---\n",
        "\n",
        "### **üñ®Ô∏è Debugging Info**\n",
        "```python\n",
        "print('Source path =', source_path, '; batch size =', batch_size)\n",
        "```\n",
        "- Prints the dataset path and batch size at the beginning of training for debugging.\n",
        "\n",
        "---\n",
        "\n",
        "### **üìè Image and Frame Configuration**\n",
        "```python\n",
        "img_idx = list(range(30))\n",
        "x = len(img_idx)\n",
        "y, z = img_size\n",
        "```\n",
        "- Use first 30 frames from each video.\n",
        "- `x = 30`, represents time steps (like LSTM sequences).\n",
        "- `y, z` define target image resolution.\n",
        "\n",
        "---\n",
        "\n",
        "### **üîÅ Infinite Loop for Continuous Batch Generation**\n",
        "```python\n",
        "while True:\n",
        "```\n",
        "- Ensures the generator keeps yielding batches as long as training continues.\n",
        "\n",
        "---\n",
        "\n",
        "### **üîÄ Shuffle Data**\n",
        "```python\n",
        "t = np.random.permutation(folder_list)\n",
        "num_batches = len(t) // batch_size\n",
        "```\n",
        "- Shuffles folder entries randomly to prevent overfitting due to fixed order.\n",
        "- Calculates how many full batches will be processed.\n",
        "\n",
        "---\n",
        "\n",
        "### **üì¶ Batch Initialization**\n",
        "```python\n",
        "batch_data = np.zeros((batch_size, x, y, z, 3))\n",
        "batch_labels = np.zeros((batch_size, num_classes))\n",
        "```\n",
        "- Preallocates memory for:\n",
        "  - `batch_data`: shape = `[batch_size, 30 frames, height, width, RGB]`\n",
        "  - `batch_labels`: shape = `[batch_size, num_classes]`, to hold one-hot encoded labels.\n",
        "\n",
        "---\n",
        "\n",
        "### **üß∑ Load and Process Each Sample in Batch**\n",
        "```python\n",
        "folder_name = t[folder + (batch * batch_size)].strip().split(';')[0]\n",
        "folder_path = os.path.join(source_path, folder_name)\n",
        "imgs = sorted(os.listdir(folder_path))\n",
        "```\n",
        "- Extracts folder name and reads all frames for a single video sample.\n",
        "- Sorts image files to ensure proper frame order.\n",
        "\n",
        "---\n",
        "\n",
        "### **üñºÔ∏è Process Each Frame in a Sample**\n",
        "```python\n",
        "image = Image.open(img_path).resize((y, z))\n",
        "image_array = np.array(image).astype(np.float32) / 255.0\n",
        "```\n",
        "- Resizes image to target size.\n",
        "- Converts pixel values to range [0, 1] for normalization.\n",
        "\n",
        "```python\n",
        "if image_array.shape[-1] == 3:\n",
        "    batch_data[folder, idx, :, :, :] = image_array\n",
        "else:\n",
        "    batch_data[folder, idx, :, :, :] = np.stack([image_array]*3, axis=-1)\n",
        "```\n",
        "- Ensures all images are in RGB format.\n",
        "- If grayscale, replicates the channel 3 times.\n",
        "\n",
        "---\n",
        "\n",
        "### **üè∑Ô∏è One-Hot Encode the Label**\n",
        "```python\n",
        "label = int(t[folder + (batch * batch_size)].strip().split(';')[2])\n",
        "batch_labels[folder] = to_categorical(label, num_classes=num_classes)\n",
        "```\n",
        "- Extracts label from line, converts it into a one-hot encoded vector.\n",
        "\n",
        "---\n",
        "\n",
        "### **üì§ Yield the Batch**\n",
        "```python\n",
        "yield batch_data, batch_labels\n",
        "```\n",
        "- Sends one batch of processed image sequences and labels to the training loop.\n",
        "\n",
        "---\n",
        "\n",
        "### **üß© Handle Remaining Samples**\n",
        "```python\n",
        "if len(t) % batch_size != 0:\n",
        "```\n",
        "- If leftover data doesn‚Äôt form a full batch, handle it similarly and yield that as a smaller batch.\n",
        "\n",
        "---\n",
        "\n",
        "### **‚úÖ Summary Table**\n",
        "\n",
        "| **Step**                     | **Description**                                                               |\n",
        "|-----------------------------|-------------------------------------------------------------------------------|\n",
        "| Shuffle folder list         | Prevents model from learning order of data                                   |\n",
        "| Initialize batch arrays     | Allocates memory for inputs and targets                                      |\n",
        "| Load frames from folder     | Reads and processes each image (frame)                                       |\n",
        "| Normalize and RGB-check     | Normalizes pixel values and ensures consistent color channels                |\n",
        "| One-hot encode labels       | Converts class index to softmax-compatible vector                            |\n",
        "| Yield batch                 | Returns batch for training, memory-efficient                                 |\n",
        "| Handle incomplete batch     | Ensures no data is left unused even if not divisible by batch size           |\n",
        "\n",
        "---\n",
        "\n",
        "### **üìä Optional Visualization**\n",
        "To visualize one sample frame:\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(batch_data[0, 0])  # First video, first frame\n",
        "plt.title(\"Sample Frame from Batch\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "```"
      ],
      "metadata": {
        "id": "dNuWhvJw5i52"
      },
      "id": "dNuWhvJw5i52"
    },
    {
      "cell_type": "markdown",
      "id": "9bf6950b",
      "metadata": {
        "id": "9bf6950b"
      },
      "source": [
        "## 3. Setting Data Paths, Sequence Counts, and Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "951efd85",
      "metadata": {
        "id": "951efd85",
        "outputId": "40c360de-98ea-4ff8-9805-ede08f9ff54f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# training sequences = 663\n",
            "# validation sequences = 100\n",
            "# epochs = 20\n",
            "Current Date and Time: 2024-11-13 09:48:29.670989\n"
          ]
        }
      ],
      "source": [
        "train_path = '/home/datasets/Project_data/train'\n",
        "val_path = '/home/datasets/Project_data/val'\n",
        "\n",
        "num_train_sequences = len(train_doc)\n",
        "print('# training sequences =', num_train_sequences)\n",
        "\n",
        "num_val_sequences = len(val_doc)\n",
        "print('# validation sequences =', num_val_sequences)\n",
        "\n",
        "num_epochs = 20\n",
        "print('# epochs =', num_epochs)\n",
        "\n",
        "curr_dt_time = datetime.datetime.now()\n",
        "print(\"Current Date and Time:\", curr_dt_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wzdoRnwh6odK"
      },
      "id": "wzdoRnwh6odK"
    },
    {
      "cell_type": "markdown",
      "id": "5b77e88b",
      "metadata": {
        "id": "5b77e88b"
      },
      "source": [
        "## 4. Building a 3D Convolutional Neural Network Model for Video Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d6f3502",
      "metadata": {
        "id": "0d6f3502",
        "outputId": "6d7722db-8f7e-403b-f8ff-3b351af91d44"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-11-13 09:48:54.616999: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2024-11-13 09:48:54.617083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14800 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:1b:00.0, compute capability: 7.5\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def create_conv3d_model(input_shape=(30, 100, 100, 3), num_classes=5):\n",
        "    model = Sequential()\n",
        "    model.add(Conv3D(32, kernel_size=(3, 3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv3D(64, kernel_size=(3, 3, 3), activation='relu'))\n",
        "    model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv3D(128, kernel_size=(3, 3, 3), activation='relu'))\n",
        "    model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "input_shape = (30, 100, 100, 3)\n",
        "num_classes = 5\n",
        "conv3d_model = create_conv3d_model(input_shape=input_shape, num_classes=num_classes)\n",
        "conv3d_model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eac3346e",
      "metadata": {
        "id": "eac3346e"
      },
      "source": [
        "## 5. Compiling the 3D CNN Model with Optimizer and Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4147efef",
      "metadata": {
        "id": "4147efef",
        "outputId": "a1187508-7ac0-446b-a4a4-ca9469bd1ada"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d (Conv3D)             (None, 28, 98, 98, 32)    2624      \n",
            "                                                                 \n",
            " max_pooling3d (MaxPooling3D  (None, 14, 49, 49, 32)   0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 14, 49, 49, 32)   128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv3d_1 (Conv3D)           (None, 12, 47, 47, 64)    55360     \n",
            "                                                                 \n",
            " max_pooling3d_1 (MaxPooling  (None, 6, 23, 23, 64)    0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 6, 23, 23, 64)    256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv3d_2 (Conv3D)           (None, 4, 21, 21, 128)    221312    \n",
            "                                                                 \n",
            " max_pooling3d_2 (MaxPooling  (None, 2, 10, 10, 128)   0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 2, 10, 10, 128)   512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25600)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               6553856   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5)                 1285      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,835,333\n",
            "Trainable params: 6,834,885\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "optimiser = Adam(learning_rate=0.001)\n",
        "\n",
        "model = conv3d_model\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dade6238",
      "metadata": {
        "id": "dade6238"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f78cecae",
      "metadata": {
        "id": "f78cecae"
      },
      "source": [
        "## 6. Setting Up Model Checkpoints and Learning Rate Scheduler for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70373fac",
      "metadata": {
        "id": "70373fac"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ', '').replace(':', '_') + '/'\n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "\n",
        "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, min_lr=1e-6)\n",
        "\n",
        "callbacks_list = [checkpoint, LR]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca8da92e",
      "metadata": {
        "id": "ca8da92e"
      },
      "source": [
        "## 7. Calculating Steps per Epoch and Validation Steps Based on Batch Size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd81ee8f",
      "metadata": {
        "id": "dd81ee8f",
        "outputId": "b81635d3-3ce9-4e0e-cd00-d5e5f7a42c9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Steps per epoch: 42\n",
            "Validation steps: 7\n"
          ]
        }
      ],
      "source": [
        "if (num_train_sequences % batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences / batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences // batch_size) + 1\n",
        "\n",
        "if (num_val_sequences % batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences / batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences // batch_size) + 1\n",
        "\n",
        "print(\"Steps per epoch:\", steps_per_epoch)\n",
        "print(\"Validation steps:\", validation_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3274ff9",
      "metadata": {
        "id": "a3274ff9"
      },
      "source": [
        "## 8. Training the Model with Generators and Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a95a8b7",
      "metadata": {
        "id": "1a95a8b7",
        "outputId": "8132e7cf-ba97-4ead-dbfb-9bf1a57f6db3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source path = /home/datasets/Project_data/train ; batch size = 16\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-11-13 09:50:07.746512: I tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Loaded cuDNN version 8302\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "42/42 [==============================] - ETA: 0s - loss: 4.4429 - categorical_accuracy: 0.4781Source path = /home/datasets/Project_data/val ; batch size = 16\n",
            "\n",
            "Epoch 00001: saving model to model_init_2024-11-1309_48_29.670989/model-00001-4.44287-0.47813-7.72766-0.19000.h5\n",
            "42/42 [==============================] - 132s 3s/step - loss: 4.4429 - categorical_accuracy: 0.4781 - val_loss: 7.7277 - val_categorical_accuracy: 0.1900 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.5869 - categorical_accuracy: 0.6968\n",
            "Epoch 00002: saving model to model_init_2024-11-1309_48_29.670989/model-00002-1.58690-0.69683-8.19304-0.14000.h5\n",
            "42/42 [==============================] - 129s 3s/step - loss: 1.5869 - categorical_accuracy: 0.6968 - val_loss: 8.1930 - val_categorical_accuracy: 0.1400 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.8599 - categorical_accuracy: 0.7783\n",
            "Epoch 00003: saving model to model_init_2024-11-1309_48_29.670989/model-00003-0.85990-0.77828-9.49507-0.24000.h5\n",
            "42/42 [==============================] - 126s 3s/step - loss: 0.8599 - categorical_accuracy: 0.7783 - val_loss: 9.4951 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.1559 - categorical_accuracy: 0.7466\n",
            "Epoch 00004: saving model to model_init_2024-11-1309_48_29.670989/model-00004-1.15587-0.74661-20.73573-0.18000.h5\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "42/42 [==============================] - 121s 3s/step - loss: 1.1559 - categorical_accuracy: 0.7466 - val_loss: 20.7357 - val_categorical_accuracy: 0.1800 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.6511 - categorical_accuracy: 0.8401\n",
            "Epoch 00005: saving model to model_init_2024-11-1309_48_29.670989/model-00005-0.65108-0.84012-9.97107-0.24000.h5\n",
            "42/42 [==============================] - 115s 3s/step - loss: 0.6511 - categorical_accuracy: 0.8401 - val_loss: 9.9711 - val_categorical_accuracy: 0.2400 - lr: 5.0000e-04\n",
            "Epoch 6/20\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.3738 - categorical_accuracy: 0.8748\n",
            "Epoch 00006: saving model to model_init_2024-11-1309_48_29.670989/model-00006-0.37383-0.87481-8.28410-0.29000.h5\n",
            "42/42 [==============================] - 108s 3s/step - loss: 0.3738 - categorical_accuracy: 0.8748 - val_loss: 8.2841 - val_categorical_accuracy: 0.2900 - lr: 5.0000e-04\n",
            "Epoch 7/20\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.2852 - categorical_accuracy: 0.9080\n",
            "Epoch 00007: saving model to model_init_2024-11-1309_48_29.670989/model-00007-0.28524-0.90799-7.80978-0.23000.h5\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "42/42 [==============================] - 104s 3s/step - loss: 0.2852 - categorical_accuracy: 0.9080 - val_loss: 7.8098 - val_categorical_accuracy: 0.2300 - lr: 5.0000e-04\n",
            "Epoch 8/20\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.2165 - categorical_accuracy: 0.9261\n",
            "Epoch 00008: saving model to model_init_2024-11-1309_48_29.670989/model-00008-0.21648-0.92609-3.44347-0.44000.h5\n",
            "42/42 [==============================] - 104s 3s/step - loss: 0.2165 - categorical_accuracy: 0.9261 - val_loss: 3.4435 - val_categorical_accuracy: 0.4400 - lr: 2.5000e-04\n",
            "Epoch 9/20\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.1810 - categorical_accuracy: 0.9442\n",
            "Epoch 00009: saving model to model_init_2024-11-1309_48_29.670989/model-00009-0.18096-0.94419-4.44292-0.44000.h5\n",
            "42/42 [==============================] - 102s 2s/step - loss: 0.1810 - categorical_accuracy: 0.9442 - val_loss: 4.4429 - val_categorical_accuracy: 0.4400 - lr: 2.5000e-04\n",
            "Epoch 10/20\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.1138 - categorical_accuracy: 0.9563\n",
            "Epoch 00010: saving model to model_init_2024-11-1309_48_29.670989/model-00010-0.11382-0.95626-4.29777-0.51000.h5\n",
            "42/42 [==============================] - 97s 2s/step - loss: 0.1138 - categorical_accuracy: 0.9563 - val_loss: 4.2978 - val_categorical_accuracy: 0.5100 - lr: 2.5000e-04\n",
            "Epoch 11/20\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.1120 - categorical_accuracy: 0.9578\n",
            "Epoch 00011: saving model to model_init_2024-11-1309_48_29.670989/model-00011-0.11202-0.95777-1.57519-0.72000.h5\n",
            "42/42 [==============================] - 99s 2s/step - loss: 0.1120 - categorical_accuracy: 0.9578 - val_loss: 1.5752 - val_categorical_accuracy: 0.7200 - lr: 2.5000e-04\n",
            "Epoch 12/20\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.1170 - categorical_accuracy: 0.9532\n",
            "Epoch 00012: saving model to model_init_2024-11-1309_48_29.670989/model-00012-0.11704-0.95324-1.29859-0.76000.h5\n",
            "42/42 [==============================] - 102s 2s/step - loss: 0.1170 - categorical_accuracy: 0.9532 - val_loss: 1.2986 - val_categorical_accuracy: 0.7600 - lr: 2.5000e-04\n",
            "Epoch 13/20\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.0963 - categorical_accuracy: 0.9683\n",
            "Epoch 00013: saving model to model_init_2024-11-1309_48_29.670989/model-00013-0.09631-0.96833-1.46585-0.79000.h5\n",
            "42/42 [==============================] - 101s 2s/step - loss: 0.0963 - categorical_accuracy: 0.9683 - val_loss: 1.4659 - val_categorical_accuracy: 0.7900 - lr: 2.5000e-04\n",
            "Epoch 14/20\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.0783 - categorical_accuracy: 0.9713\n",
            "Epoch 00014: saving model to model_init_2024-11-1309_48_29.670989/model-00014-0.07831-0.97134-1.60355-0.81000.h5\n",
            "42/42 [==============================] - 101s 2s/step - loss: 0.0783 - categorical_accuracy: 0.9713 - val_loss: 1.6036 - val_categorical_accuracy: 0.8100 - lr: 2.5000e-04\n",
            "Epoch 15/20\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.1228 - categorical_accuracy: 0.9608\n",
            "Epoch 00015: saving model to model_init_2024-11-1309_48_29.670989/model-00015-0.12278-0.96078-1.32910-0.82000.h5\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "42/42 [==============================] - 103s 3s/step - loss: 0.1228 - categorical_accuracy: 0.9608 - val_loss: 1.3291 - val_categorical_accuracy: 0.8200 - lr: 2.5000e-04\n",
            "Epoch 16/20\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.0813 - categorical_accuracy: 0.9698\n",
            "Epoch 00016: saving model to model_init_2024-11-1309_48_29.670989/model-00016-0.08134-0.96983-2.01950-0.78000.h5\n",
            "42/42 [==============================] - 104s 3s/step - loss: 0.0813 - categorical_accuracy: 0.9698 - val_loss: 2.0195 - val_categorical_accuracy: 0.7800 - lr: 1.2500e-04\n",
            "Epoch 17/20\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.0661 - categorical_accuracy: 0.9744\n",
            "Epoch 00017: saving model to model_init_2024-11-1309_48_29.670989/model-00017-0.06609-0.97436-1.18475-0.83000.h5\n",
            "42/42 [==============================] - 107s 3s/step - loss: 0.0661 - categorical_accuracy: 0.9744 - val_loss: 1.1847 - val_categorical_accuracy: 0.8300 - lr: 1.2500e-04\n",
            "Epoch 18/20\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.0557 - categorical_accuracy: 0.9759\n",
            "Epoch 00018: saving model to model_init_2024-11-1309_48_29.670989/model-00018-0.05569-0.97587-1.43985-0.85000.h5\n",
            "42/42 [==============================] - 106s 3s/step - loss: 0.0557 - categorical_accuracy: 0.9759 - val_loss: 1.4399 - val_categorical_accuracy: 0.8500 - lr: 1.2500e-04\n",
            "Epoch 19/20\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.0450 - categorical_accuracy: 0.9849\n",
            "Epoch 00019: saving model to model_init_2024-11-1309_48_29.670989/model-00019-0.04505-0.98492-0.91332-0.85000.h5\n",
            "42/42 [==============================] - 105s 3s/step - loss: 0.0450 - categorical_accuracy: 0.9849 - val_loss: 0.9133 - val_categorical_accuracy: 0.8500 - lr: 1.2500e-04\n",
            "Epoch 20/20\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.0488 - categorical_accuracy: 0.9774\n",
            "Epoch 00020: saving model to model_init_2024-11-1309_48_29.670989/model-00020-0.04882-0.97738-0.89363-0.83000.h5\n",
            "42/42 [==============================] - 106s 3s/step - loss: 0.0488 - categorical_accuracy: 0.9774 - val_loss: 0.8936 - val_categorical_accuracy: 0.8300 - lr: 1.2500e-04\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff2bce4f0a0>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=num_epochs,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks_list,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=validation_steps,\n",
        "    class_weight=None,\n",
        "    workers=1,\n",
        "    initial_epoch=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1684554d",
      "metadata": {
        "id": "1684554d"
      },
      "source": [
        "## 9.Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32830aed",
      "metadata": {
        "id": "32830aed",
        "outputId": "a50d67fa-904a-49b0-c267-80d7b7682cc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7/7 [==============================] - 14s 2s/step - loss: 0.9154 - categorical_accuracy: 0.8500\n",
            "Validation Loss: 0.9153631329536438\n",
            "Validation Accuracy: 0.8500000238418579\n"
          ]
        }
      ],
      "source": [
        "val_loss, val_accuracy = model.evaluate(val_generator, steps=validation_steps)\n",
        "print(f'Validation Loss: {val_loss}')\n",
        "print(f'Validation Accuracy: {val_accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35b93cb0",
      "metadata": {
        "id": "35b93cb0"
      },
      "source": [
        "## 10. Model Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d46354d",
      "metadata": {
        "id": "1d46354d"
      },
      "outputs": [],
      "source": [
        "model.save(\"final_model.h5\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}